{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 23/08/19\n",
    "\n",
    "The dataset used for NN is captured in the lab environment. The appliances used are Acer Laptop Charger and Electric Fan\n",
    "\n",
    "The data below are as indicated.\n",
    "\n",
    "Status\n",
    "\n",
    "00 0 Off State\n",
    "\n",
    "01 1 Fan On\n",
    "\n",
    "10 2 Laptop On\n",
    "\n",
    "11 3 Laptop + Fan\n",
    "\n",
    "Initialize weights wij with arbitrary values\n",
    "\n",
    "Repeat\n",
    "        \n",
    "        Pick a training example <x,oˇ> (x is the input, oˇ is the expected output)\n",
    "        Compute the sum for each neuron: Sj=∑i=1Nwijxi\n",
    "        Compute outputs (o) of the network: oj=f(S)\n",
    "        For each output, compute the error: δj=(oˇj−oj)\n",
    "        Update each synaptic weights of the network: wij=wij+η.δj.xi.df(S)dS\n",
    "Until the training set is empty\n",
    "\n",
    "Date: 28/08/19\n",
    "Changes made from previous version 1.2\n",
    "1. Reshape Y data and normalize it properly\n",
    "2. Retrain the model and extract the prediction output into csv to visualize\n",
    "\n",
    "a. The output with the respective model is still wrong refer to the csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aggregate Power</th>\n",
       "      <th>Aggregate Current</th>\n",
       "      <th>Calculated Aggregated Current</th>\n",
       "      <th>Calculated Aggregated Power</th>\n",
       "      <th>Fan</th>\n",
       "      <th>Laptop</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "      <td>10613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.373222</td>\n",
       "      <td>0.349449</td>\n",
       "      <td>0.400326</td>\n",
       "      <td>92.074984</td>\n",
       "      <td>0.164479</td>\n",
       "      <td>0.235847</td>\n",
       "      <td>2.560539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.941137</td>\n",
       "      <td>0.186701</td>\n",
       "      <td>0.188029</td>\n",
       "      <td>43.246625</td>\n",
       "      <td>0.098429</td>\n",
       "      <td>0.139470</td>\n",
       "      <td>0.889290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.900000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>55.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.600000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.700000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>121.900000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>177.100000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Aggregate Power  Aggregate Current  Calculated Aggregated Current  \\\n",
       "count     10613.000000       10613.000000                   10613.000000   \n",
       "mean         80.373222           0.349449                       0.400326   \n",
       "std          42.941137           0.186701                       0.188029   \n",
       "min           0.000000           0.000000                       0.000000   \n",
       "25%          52.900000           0.230000                       0.240000   \n",
       "50%          96.600000           0.420000                       0.500000   \n",
       "75%         112.700000           0.490000                       0.530000   \n",
       "max         161.000000           0.700000                       0.770000   \n",
       "\n",
       "       Calculated Aggregated Power           Fan        Laptop        Status  \n",
       "count                 10613.000000  10613.000000  10613.000000  10613.000000  \n",
       "mean                     92.074984      0.164479      0.235847      2.560539  \n",
       "std                      43.246625      0.098429      0.139470      0.889290  \n",
       "min                       0.000000      0.000000      0.000000      0.000000  \n",
       "25%                      55.200000      0.000000      0.200000      3.000000  \n",
       "50%                     115.000000      0.220000      0.290000      3.000000  \n",
       "75%                     121.900000      0.230000      0.320000      3.000000  \n",
       "max                     177.100000      0.280000      0.540000      3.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'C:\\Users\\shich\\Desktop\\FYP\\Documentation\\Data Acquisition\\Lab\\Profile#1.csv')#Reading the house price data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.61e+01, 7.00e-02, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],\n",
       "       [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],\n",
       "       [2.30e+00, 1.00e-02, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],\n",
       "       ...,\n",
       "       [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],\n",
       "       [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],\n",
       "       [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting in the arrays so that our machine could process\n",
    "dataset = df.values\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the our values are stored in the array now.\n",
    "\n",
    "Now we will split our dataset into input features (X) and the feature we wish to predict (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10613, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y= dataset[:,6:7]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10613, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "Y_shape = scaler.fit_transform(Y)\n",
    "#Y_series = pd.Series(Y)\n",
    "Y_df = pd.DataFrame(Y_shape)\n",
    "#This is to see the full result of after conversion\n",
    "#Y_df.to_csv('new.csv')\n",
    "Y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensuring that scaling of our input features are similar with the help of sckit-learn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#using the function 'min-max scaler' which scales the dataset so that\n",
    "#all the features lie between 0 and 1\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7429, 3) (1592, 3) (1592, 3) (7429, 1) (1592, 1) (1592, 1)\n"
     ]
    }
   ],
   "source": [
    "#We will now split our data into the training set and test set with the help of the sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_df, test_size=0.3)\n",
    "#This tell sklearn that your_val_and_test size will be 30% of the overall dataset.\n",
    "\n",
    "#using the same function to do the split again on val_and_test\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, We have 7429 data points while the validation and test set has 1592 data points each.\n",
    "#### Till now we have done following things step-wise\n",
    "1. Read in the CSV file\n",
    "2. Split our dataset into the input features and the label\n",
    "3. Scale the data so that the input features have similar orders of the magnitude.\n",
    "4. Splitting our dataset into the training set, the validation set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building And Training The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Visualization purpose\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to have have follwing kind of layers in the Neural Network\n",
    "1. Hidden Layer 1:32 neurons, ReLU activation\n",
    "2. Hidden Layer 2:32 neurons, ReLU activation\n",
    "3. Output Layer 1 neuron, Sigmoid activation\n",
    "\n",
    "We will run the follwoing code to fulfill our requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 20:53:48.706621 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 20:53:48.723034 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 20:53:48.725190 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,217\n",
      "Trainable params: 1,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#This says that we will store our model in the variable 'model' and we will\n",
    "#describe it sequentially(layer by layer)\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(3,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the best numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 20:53:49.114609 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 20:53:49.163618 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0828 20:53:49.171199 21960 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,217\n",
      "Trainable params: 1,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model.compile(optimizer='sgd',\n",
    "#             loss='binary_crossentropy',\n",
    "#             metrics=['accuracy'])\n",
    "#sgd means stochastic gradient descent\n",
    "#loss function for outputs that \n",
    "#metrics=accuracy means we want to track accuracy on top of the loss function\n",
    "model.compile(optimizer='sgd',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 20:53:49.433467 21960 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7429 samples, validate on 1592 samples\n",
      "Epoch 1/100\n",
      "7429/7429 [==============================] - 1s 81us/step - loss: 0.4352 - acc: 0.7586 - val_loss: 0.3271 - val_acc: 0.7707\n",
      "Epoch 2/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.3224 - acc: 0.7582 - val_loss: 0.2963 - val_acc: 0.7707\n",
      "Epoch 3/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2988 - acc: 0.7582 - val_loss: 0.2758 - val_acc: 0.7707\n",
      "Epoch 4/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2783 - acc: 0.7582 - val_loss: 0.2583 - val_acc: 0.7707\n",
      "Epoch 5/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2635 - acc: 0.7582 - val_loss: 0.2479 - val_acc: 0.7707\n",
      "Epoch 6/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2553 - acc: 0.7770 - val_loss: 0.2418 - val_acc: 0.8260\n",
      "Epoch 7/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.2501 - acc: 0.8203 - val_loss: 0.2370 - val_acc: 0.8260\n",
      "Epoch 8/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2455 - acc: 0.8203 - val_loss: 0.2328 - val_acc: 0.8260\n",
      "Epoch 9/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2415 - acc: 0.8203 - val_loss: 0.2290 - val_acc: 0.8260\n",
      "Epoch 10/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2379 - acc: 0.8203 - val_loss: 0.2256 - val_acc: 0.8260\n",
      "Epoch 11/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2348 - acc: 0.8203 - val_loss: 0.2226 - val_acc: 0.8260\n",
      "Epoch 12/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2320 - acc: 0.8203 - val_loss: 0.2200 - val_acc: 0.8260\n",
      "Epoch 13/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2297 - acc: 0.8203 - val_loss: 0.2177 - val_acc: 0.8260\n",
      "Epoch 14/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2276 - acc: 0.8203 - val_loss: 0.2157 - val_acc: 0.8260\n",
      "Epoch 15/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2258 - acc: 0.8203 - val_loss: 0.2139 - val_acc: 0.8260\n",
      "Epoch 16/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2243 - acc: 0.8203 - val_loss: 0.2125 - val_acc: 0.8260\n",
      "Epoch 17/100\n",
      "7429/7429 [==============================] - 0s 51us/step - loss: 0.2230 - acc: 0.8203 - val_loss: 0.2113 - val_acc: 0.8260\n",
      "Epoch 18/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2219 - acc: 0.8203 - val_loss: 0.2102 - val_acc: 0.8260\n",
      "Epoch 19/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2210 - acc: 0.8203 - val_loss: 0.2094 - val_acc: 0.8260\n",
      "Epoch 20/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2204 - acc: 0.8203 - val_loss: 0.2087 - val_acc: 0.8260\n",
      "Epoch 21/100\n",
      "7429/7429 [==============================] - 0s 42us/step - loss: 0.2198 - acc: 0.8203 - val_loss: 0.2081 - val_acc: 0.8260\n",
      "Epoch 22/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2193 - acc: 0.8203 - val_loss: 0.2076 - val_acc: 0.8260\n",
      "Epoch 23/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2188 - acc: 0.8203 - val_loss: 0.2071 - val_acc: 0.8260\n",
      "Epoch 24/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2184 - acc: 0.8203 - val_loss: 0.2067 - val_acc: 0.8260\n",
      "Epoch 25/100\n",
      "7429/7429 [==============================] - 0s 39us/step - loss: 0.2180 - acc: 0.8203 - val_loss: 0.2068 - val_acc: 0.8260\n",
      "Epoch 26/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2179 - acc: 0.8203 - val_loss: 0.2062 - val_acc: 0.8260\n",
      "Epoch 27/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2177 - acc: 0.8203 - val_loss: 0.2059 - val_acc: 0.8260\n",
      "Epoch 28/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.2175 - acc: 0.8203 - val_loss: 0.2058 - val_acc: 0.8260\n",
      "Epoch 29/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2172 - acc: 0.8203 - val_loss: 0.2059 - val_acc: 0.8260\n",
      "Epoch 30/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2172 - acc: 0.8203 - val_loss: 0.2054 - val_acc: 0.8260\n",
      "Epoch 31/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2171 - acc: 0.8203 - val_loss: 0.2052 - val_acc: 0.8260\n",
      "Epoch 32/100\n",
      "7429/7429 [==============================] - 0s 42us/step - loss: 0.2169 - acc: 0.8203 - val_loss: 0.2052 - val_acc: 0.8260\n",
      "Epoch 33/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2168 - acc: 0.8203 - val_loss: 0.2053 - val_acc: 0.8260\n",
      "Epoch 34/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2168 - acc: 0.8203 - val_loss: 0.2050 - val_acc: 0.8260\n",
      "Epoch 35/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2167 - acc: 0.8203 - val_loss: 0.2048 - val_acc: 0.8260\n",
      "Epoch 36/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.2166 - acc: 0.8203 - val_loss: 0.2048 - val_acc: 0.8260\n",
      "Epoch 37/100\n",
      "7429/7429 [==============================] - 0s 42us/step - loss: 0.2166 - acc: 0.8203 - val_loss: 0.2048 - val_acc: 0.8260\n",
      "Epoch 38/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2165 - acc: 0.8203 - val_loss: 0.2046 - val_acc: 0.8260\n",
      "Epoch 39/100\n",
      "7429/7429 [==============================] - 0s 37us/step - loss: 0.2164 - acc: 0.8203 - val_loss: 0.2048 - val_acc: 0.8260\n",
      "Epoch 40/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2164 - acc: 0.8203 - val_loss: 0.2045 - val_acc: 0.8260\n",
      "Epoch 41/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.2164 - acc: 0.8203 - val_loss: 0.2045 - val_acc: 0.8260\n",
      "Epoch 42/100\n",
      "7429/7429 [==============================] - 0s 42us/step - loss: 0.2164 - acc: 0.8203 - val_loss: 0.2046 - val_acc: 0.8260\n",
      "Epoch 43/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2164 - acc: 0.8203 - val_loss: 0.2044 - val_acc: 0.8260\n",
      "Epoch 44/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2163 - acc: 0.8203 - val_loss: 0.2043 - val_acc: 0.8260\n",
      "Epoch 45/100\n",
      "7429/7429 [==============================] - 0s 33us/step - loss: 0.2162 - acc: 0.8203 - val_loss: 0.2043 - val_acc: 0.8260\n",
      "Epoch 46/100\n",
      "7429/7429 [==============================] - 0s 43us/step - loss: 0.2162 - acc: 0.8203 - val_loss: 0.2043 - val_acc: 0.8260\n",
      "Epoch 47/100\n",
      "7429/7429 [==============================] - 0s 39us/step - loss: 0.2161 - acc: 0.8203 - val_loss: 0.2044 - val_acc: 0.8260\n",
      "Epoch 48/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2161 - acc: 0.8203 - val_loss: 0.2042 - val_acc: 0.8260\n",
      "Epoch 49/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2160 - acc: 0.8203 - val_loss: 0.2041 - val_acc: 0.8260\n",
      "Epoch 50/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2161 - acc: 0.8203 - val_loss: 0.2041 - val_acc: 0.8260\n",
      "Epoch 51/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2160 - acc: 0.8203 - val_loss: 0.2041 - val_acc: 0.8260\n",
      "Epoch 52/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2160 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 53/100\n",
      "7429/7429 [==============================] - 0s 42us/step - loss: 0.2160 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 54/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2159 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 55/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2158 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 56/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2159 - acc: 0.8203 - val_loss: 0.2039 - val_acc: 0.8260\n",
      "Epoch 57/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2159 - acc: 0.8203 - val_loss: 0.2039 - val_acc: 0.8260\n",
      "Epoch 58/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2158 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 59/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2158 - acc: 0.8203 - val_loss: 0.2041 - val_acc: 0.8260\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2158 - acc: 0.8203 - val_loss: 0.2038 - val_acc: 0.8260\n",
      "Epoch 61/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2157 - acc: 0.8203 - val_loss: 0.2038 - val_acc: 0.8260\n",
      "Epoch 62/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2157 - acc: 0.8203 - val_loss: 0.2038 - val_acc: 0.8260\n",
      "Epoch 63/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2156 - acc: 0.8203 - val_loss: 0.2040 - val_acc: 0.8260\n",
      "Epoch 64/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2157 - acc: 0.8203 - val_loss: 0.2037 - val_acc: 0.8260\n",
      "Epoch 65/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2156 - acc: 0.8203 - val_loss: 0.2037 - val_acc: 0.8260\n",
      "Epoch 66/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2156 - acc: 0.8203 - val_loss: 0.2036 - val_acc: 0.8260\n",
      "Epoch 67/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2156 - acc: 0.8203 - val_loss: 0.2037 - val_acc: 0.8260\n",
      "Epoch 68/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2155 - acc: 0.8203 - val_loss: 0.2036 - val_acc: 0.8260\n",
      "Epoch 69/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2155 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 70/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2155 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 71/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2154 - acc: 0.8203 - val_loss: 0.2037 - val_acc: 0.8260\n",
      "Epoch 72/100\n",
      "7429/7429 [==============================] - 0s 37us/step - loss: 0.2154 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 73/100\n",
      "7429/7429 [==============================] - 0s 33us/step - loss: 0.2153 - acc: 0.8203 - val_loss: 0.2036 - val_acc: 0.8260\n",
      "Epoch 74/100\n",
      "7429/7429 [==============================] - 0s 35us/step - loss: 0.2153 - acc: 0.8203 - val_loss: 0.2034 - val_acc: 0.8260\n",
      "Epoch 75/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2153 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 76/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2153 - acc: 0.8203 - val_loss: 0.2034 - val_acc: 0.8260\n",
      "Epoch 77/100\n",
      "7429/7429 [==============================] - 0s 56us/step - loss: 0.2152 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 78/100\n",
      "7429/7429 [==============================] - 0s 51us/step - loss: 0.2152 - acc: 0.8203 - val_loss: 0.2033 - val_acc: 0.8260\n",
      "Epoch 79/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2151 - acc: 0.8203 - val_loss: 0.2035 - val_acc: 0.8260\n",
      "Epoch 80/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2152 - acc: 0.8203 - val_loss: 0.2033 - val_acc: 0.8260\n",
      "Epoch 81/100\n",
      "7429/7429 [==============================] - 0s 50us/step - loss: 0.2151 - acc: 0.8203 - val_loss: 0.2032 - val_acc: 0.8260\n",
      "Epoch 82/100\n",
      "7429/7429 [==============================] - 0s 48us/step - loss: 0.2151 - acc: 0.8203 - val_loss: 0.2032 - val_acc: 0.8260\n",
      "Epoch 83/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2151 - acc: 0.8203 - val_loss: 0.2034 - val_acc: 0.8260\n",
      "Epoch 84/100\n",
      "7429/7429 [==============================] - 0s 39us/step - loss: 0.2150 - acc: 0.8203 - val_loss: 0.2031 - val_acc: 0.8260\n",
      "Epoch 85/100\n",
      "7429/7429 [==============================] - 0s 40us/step - loss: 0.2150 - acc: 0.8203 - val_loss: 0.2031 - val_acc: 0.8260\n",
      "Epoch 86/100\n",
      "7429/7429 [==============================] - 0s 46us/step - loss: 0.2149 - acc: 0.8203 - val_loss: 0.2031 - val_acc: 0.8260\n",
      "Epoch 87/100\n",
      "7429/7429 [==============================] - 0s 41us/step - loss: 0.2149 - acc: 0.8203 - val_loss: 0.2030 - val_acc: 0.8260\n",
      "Epoch 88/100\n",
      "7429/7429 [==============================] - 0s 39us/step - loss: 0.2149 - acc: 0.8203 - val_loss: 0.2031 - val_acc: 0.8260\n",
      "Epoch 89/100\n",
      "7429/7429 [==============================] - 0s 45us/step - loss: 0.2149 - acc: 0.8203 - val_loss: 0.2030 - val_acc: 0.8260\n",
      "Epoch 90/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2148 - acc: 0.8203 - val_loss: 0.2030 - val_acc: 0.8260\n",
      "Epoch 91/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2148 - acc: 0.8203 - val_loss: 0.2030 - val_acc: 0.8260\n",
      "Epoch 92/100\n",
      "7429/7429 [==============================] - 0s 47us/step - loss: 0.2148 - acc: 0.8203 - val_loss: 0.2029 - val_acc: 0.8260\n",
      "Epoch 93/100\n",
      "7429/7429 [==============================] - 0s 50us/step - loss: 0.2147 - acc: 0.8203 - val_loss: 0.2029 - val_acc: 0.8260\n",
      "Epoch 94/100\n",
      "7429/7429 [==============================] - 0s 51us/step - loss: 0.2147 - acc: 0.8203 - val_loss: 0.2028 - val_acc: 0.8260\n",
      "Epoch 95/100\n",
      "7429/7429 [==============================] - 0s 54us/step - loss: 0.2147 - acc: 0.8203 - val_loss: 0.2028 - val_acc: 0.8260\n",
      "Epoch 96/100\n",
      "7429/7429 [==============================] - 0s 50us/step - loss: 0.2147 - acc: 0.8203 - val_loss: 0.2028 - val_acc: 0.8260\n",
      "Epoch 97/100\n",
      "7429/7429 [==============================] - 0s 52us/step - loss: 0.2145 - acc: 0.8203 - val_loss: 0.2029 - val_acc: 0.8260\n",
      "Epoch 98/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2146 - acc: 0.8203 - val_loss: 0.2027 - val_acc: 0.8260\n",
      "Epoch 99/100\n",
      "7429/7429 [==============================] - 0s 49us/step - loss: 0.2145 - acc: 0.8203 - val_loss: 0.2027 - val_acc: 0.8260\n",
      "Epoch 100/100\n",
      "7429/7429 [==============================] - 0s 44us/step - loss: 0.2145 - acc: 0.8203 - val_loss: 0.2027 - val_acc: 0.8260\n"
     ]
    }
   ],
   "source": [
    "#By the function fit we are fitting the parameters to the data\n",
    "hist = model.fit(X_train, Y_train,\n",
    "                batch_size=32, epochs=100,\n",
    "                validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, from the previous code we were able to see that the loss started off extremely high and depreciate insignificantly to allievate this issue I map the output to 0 and 1 then re run the model again. We get a more desirable range of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592/1592 [==============================] - 0s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8316582914572864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating on the test set\n",
    "model.evaluate(X_test, Y_test)[1]#Using index 1 because the function returns the loss as the first element and the accuracy as the second element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592/1592 [==============================] - 0s 41us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7bebae2e688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Test the model on the existing data set that have been split into train and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "#Test the model on the existing data set that have been split into train and test\n",
    "prediction = model.predict(X_test,verbose=1)\n",
    "prediction = scaler.inverse_transform(predictions)\n",
    "prediction_df=pd.DataFrame(prediction)\n",
    "prediction_df.to_csv('new.csv')\n",
    "\n",
    "#Output X_test to verify the result of the output\n",
    "X_testdf = pd.DataFrame(X_test)\n",
    "X_testdf.to_csv('new1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss And Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])#plotting the loss\n",
    "plt.plot(hist.history['val_loss'])#plotting the val_loss\n",
    "plt.title('Model loss')#Title of the graph\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential([\n",
    "    Dense(1000, activation='relu', input_shape=(3,)),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "hist_2 = model_2.fit(X_train, Y_train,\n",
    "                    batch_size=32, epochs=50,\n",
    "                    validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_2.history['loss'])\n",
    "plt.plot(hist_2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss is decreasing, but the validation loss is way above the training loss and increasing hence we can see that this is the clear sign of the over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy code\n",
    "plt.plot(hist_2.history['acc'])\n",
    "plt.plot(hist_2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing over-fitting with L2 regularization and dropout\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "#kernel_regularizer tells Keras to include the squared values of those parameters\n",
    "\n",
    "#our overall loss function, and weight them by 0.01 in the loss function\n",
    "model_3 = Sequential([\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(3,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
    "])\n",
    "\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "hist_3 = model_3.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=50,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising validation and training loss\n",
    "plt.plot(hist_3.history['loss'])\n",
    "plt.plot(hist_3.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.ylim(top=1.2, bottom=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the accuracy\n",
    "plt.plot(hist_3.history['acc'])\n",
    "plt.plot(hist_3.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
